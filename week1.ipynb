{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76eaf01",
   "metadata": {},
   "source": [
    "# Basic Linear Algebra in PyTorch\n",
    "\n",
    "Linear algebra forms the computational backbone of modern machine learning. In this notebook, we'll explore how PyTorch implements these operations efficiently, using temperature data as our running example.\n",
    "\n",
    "Three key ideas drive PyTorch's design:\n",
    "1. Tensors extend vectors and matrices to arbitrary dimensions, enabling batch processing\n",
    "2. Memory layout and broadcasting optimize computation through cache-friendly operations\n",
    "3. SVD reveals low-dimensional structure in high-dimensional data\n",
    "\n",
    "These concepts power everything from computer vision to natural language processing, but we'll build intuition through a simpler domain: temperature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf1be58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:13.355399Z",
     "iopub.status.busy": "2025-01-09T15:33:13.355274Z",
     "iopub.status.idle": "2025-01-09T15:33:14.406997Z",
     "shell.execute_reply": "2025-01-09T15:33:14.406663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default tensor type for consistent precision\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Utility function for consistent printing\n",
    "def print_tensor(name, x):\n",
    "    \"\"\"Print tensor with name and shape.\"\"\"\n",
    "    print(f\"{name}: shape={x.shape}\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7b35f",
   "metadata": {},
   "source": [
    "## Vectors and Tensors: The Foundation\n",
    "    \n",
    "Vectors represent sequences of measurements. While mathematical vectors are abstract, computational vectors must handle real-world constraints like memory layout and numerical precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a735c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.409032Z",
     "iopub.status.busy": "2025-01-09T15:33:14.408885Z",
     "iopub.status.idle": "2025-01-09T15:33:14.412408Z",
     "shell.execute_reply": "2025-01-09T15:33:14.412096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readings: tensor([22.5000, 23.1000, 21.8000])\n",
      "Shape: torch.Size([3])\n",
      "Data type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings (Celsius)\n",
    "readings = torch.tensor([22.5, 23.1, 21.8])  # Morning, noon, night\n",
    "print(f\"Readings: {readings}\")\n",
    "print(f\"Shape: {readings.shape}\")\n",
    "print(f\"Data type: {readings.dtype}\")  # PyTorch chooses optimal precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e74f63",
   "metadata": {},
   "source": [
    "PyTorch implements vector operations through SIMD (Single Instruction Multiple Data) parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068f9a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.414113Z",
     "iopub.status.busy": "2025-01-09T15:33:14.414023Z",
     "iopub.status.idle": "2025-01-09T15:33:14.417063Z",
     "shell.execute_reply": "2025-01-09T15:33:14.416769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: tensor([43.5000, 45.6000, 42.7000])\n",
      "Weighted: tensor([11.2500, 11.5500, 10.9000])\n"
     ]
    }
   ],
   "source": [
    "# Compare two days\n",
    "morning = torch.tensor([22.5, 23.1, 21.8])  # Yesterday\n",
    "evening = torch.tensor([21.0, 22.5, 20.9])  # Today\n",
    "\n",
    "# Vector operations - each element processed in parallel\n",
    "total = morning + evening\n",
    "print(f\"Sum: {total}\")\n",
    "\n",
    "alpha = 0.5  # Averaging weight\n",
    "weighted = alpha * morning  # Vectorized scalar multiplication\n",
    "print(f\"Weighted: {weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe946f",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "    \n",
    "PyTorch provides multiple tensor creation methods, each optimized for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739c01dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.418800Z",
     "iopub.status.busy": "2025-01-09T15:33:14.418679Z",
     "iopub.status.idle": "2025-01-09T15:33:14.421096Z",
     "shell.execute_reply": "2025-01-09T15:33:14.420822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: torch.Size([3])\n",
      "Matrix shape: torch.Size([7, 3])\n",
      "Memory layout: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Vector creation methods\n",
    "temps = torch.tensor([22.5, 23.1, 21.8])     # From data - copies input\n",
    "zeros = torch.zeros(3)                        # Initialized - contiguous memory\n",
    "weekly = torch.randn(7, 3)                    # Random normal - vectorized generation\n",
    "\n",
    "print(f\"Vector shape: {temps.shape}\")\n",
    "print(f\"Matrix shape: {weekly.shape}\")\n",
    "print(f\"Memory layout: {weekly.stride()}\")    # Shows how data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d978",
   "metadata": {},
   "source": [
    "### Vector Operations\n",
    "    \n",
    "Key operations combine computational efficiency with mathematical elegance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596b11b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.422778Z",
     "iopub.status.busy": "2025-01-09T15:33:14.422678Z",
     "iopub.status.idle": "2025-01-09T15:33:14.425600Z",
     "shell.execute_reply": "2025-01-09T15:33:14.425310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1447.9\n",
      "Day 1 magnitude: 38.9\n",
      "Day 2 magnitude: 37.2\n",
      "Pattern similarity: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Analyzing temperature patterns\n",
    "day1 = torch.tensor([22.5, 23.1, 21.8])  # Warmer day\n",
    "day2 = torch.tensor([21.0, 22.5, 20.9])  # Cooler day\n",
    "\n",
    "# Pattern similarity through optimized BLAS operations\n",
    "similarity = torch.dot(day1, day2)        # Uses hardware-optimized dot product\n",
    "\n",
    "# L2 norms computed efficiently through BLAS\n",
    "mag1 = torch.norm(day1, p=2)              # Stable computation via scaling\n",
    "mag2 = torch.norm(day2, p=2)              # Explicit L2 norm\n",
    "\n",
    "# Cosine similarity - numerically stable implementation\n",
    "cos_theta = similarity / (mag1 * mag2)\n",
    "\n",
    "print(f\"Similarity: {similarity:.1f}\")\n",
    "print(f\"Day 1 magnitude: {mag1:.1f}\")\n",
    "print(f\"Day 2 magnitude: {mag2:.1f}\")\n",
    "print(f\"Pattern similarity: {cos_theta:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3969c98",
   "metadata": {},
   "source": [
    "The high cosine value (near 1) reveals that temperature patterns remain consistent even as absolute values shift. This stability reflects fundamental physical constraints on daily temperature variations.\n",
    "\n",
    "### Quick Check: Vector Operations\n",
    "    \n",
    "Compute the average deviation from mean - a key statistical measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd56d6c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.427264Z",
     "iopub.status.busy": "2025-01-09T15:33:14.427143Z",
     "iopub.status.idle": "2025-01-09T15:33:14.429656Z",
     "shell.execute_reply": "2025-01-09T15:33:14.429384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deviation: 0.3067\n"
     ]
    }
   ],
   "source": [
    "readings = torch.tensor([22.5, 23.1, 21.8])\n",
    "mean = readings.mean()                                    # Stable one-pass algorithm\n",
    "deviations = readings - mean                             # Vectorized subtraction\n",
    "magnitude = torch.sqrt(torch.dot(deviations, deviations))  # Numerically stable norm\n",
    "print(f\"Average deviation: {magnitude/3:.4f}\")  # Should be around 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bd6f5",
   "metadata": {},
   "source": [
    "## Matrix Operations\n",
    "\n",
    "Matrices enable batch processing of multiple measurements. PyTorch optimizes matrix operations through:\n",
    "1. Cache-friendly memory layouts\n",
    "2. Hardware-accelerated BLAS routines\n",
    "3. Automatic operation fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443624a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.431379Z",
     "iopub.status.busy": "2025-01-09T15:33:14.431265Z",
     "iopub.status.idle": "2025-01-09T15:33:14.433801Z",
     "shell.execute_reply": "2025-01-09T15:33:14.433509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([7, 3])\n",
      "Strides: (3, 1)\n",
      "Total elements: 21\n"
     ]
    }
   ],
   "source": [
    "# One week of temperature readings (7 days × 3 times per day)\n",
    "week_temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Monday\n",
    "    [21.0, 22.5, 20.9],  # Tuesday\n",
    "    [23.1, 24.0, 22.8],  # Wednesday\n",
    "    [22.8, 23.5, 21.9],  # Thursday\n",
    "    [21.5, 22.8, 21.2],  # Friday\n",
    "    [20.9, 21.8, 20.5],  # Saturday\n",
    "    [21.2, 22.0, 20.8]   # Sunday\n",
    "])\n",
    "print(f\"Shape: {week_temps.shape}\")          # Logical structure\n",
    "print(f\"Strides: {week_temps.stride()}\")     # Physical memory layout\n",
    "print(f\"Total elements: {week_temps.numel()}\") # Number of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3c902",
   "metadata": {},
   "source": [
    "### Basic Matrix Operations\n",
    "    \n",
    "PyTorch fuses multiple operations for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a945ab39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.435682Z",
     "iopub.status.busy": "2025-01-09T15:33:14.435569Z",
     "iopub.status.idle": "2025-01-09T15:33:14.438763Z",
     "shell.execute_reply": "2025-01-09T15:33:14.438466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature changes:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Average temperatures:\n",
      "tensor([22.2000, 23.2000, 21.8333])\n"
     ]
    }
   ],
   "source": [
    "# Compare weeks with fused operations\n",
    "last_week = torch.tensor([\n",
    "    [21.5, 22.1, 20.8],  # Morning, noon, night\n",
    "    [20.0, 21.5, 19.9],\n",
    "    [22.1, 23.0, 21.8]\n",
    "])\n",
    "\n",
    "this_week = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],\n",
    "    [21.0, 22.5, 20.9],\n",
    "    [23.1, 24.0, 22.8]\n",
    "])\n",
    "\n",
    "# Temperature changes - single fused operation\n",
    "temp_change = this_week - last_week  # No temporary storage needed\n",
    "print(\"Temperature changes:\")\n",
    "print(temp_change)\n",
    "\n",
    "# Efficient reduction along specified axis\n",
    "daily_means = this_week.mean(dim=0)  # Uses stable online algorithm\n",
    "print(\"\\nAverage temperatures:\")\n",
    "print(daily_means)  # Morning, Noon, Night averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b92c59",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "    \n",
    "Matrix multiplication leverages highly optimized BLAS (Basic Linear Algebra Subprograms):\n",
    "\n",
    "![Matrix Multiplication](figures/matrix_multiply.png)\n",
    "\n",
    "The operation is optimized through:\n",
    "1. Cache blocking for memory efficiency\n",
    "2. SIMD vectorization for parallel computation\n",
    "3. Multi-threading for large matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5f82bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.440314Z",
     "iopub.status.busy": "2025-01-09T15:33:14.440234Z",
     "iopub.status.idle": "2025-01-09T15:33:14.443014Z",
     "shell.execute_reply": "2025-01-09T15:33:14.442714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted averages per time:\n",
      "tensor([22.5400, 21.4300, 23.3100])\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings and importance weights\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8]   # Day 3\n",
    "], dtype=torch.float32)  # Specify precision for BLAS\n",
    "\n",
    "weights = torch.tensor([0.5, 0.3, 0.2])  # Recent days matter more\n",
    "\n",
    "# Efficient matrix-vector multiply using BLAS\n",
    "weighted_means = torch.mv(temps, weights)  # Optimized for matrix-vector product\n",
    "print(\"Weighted averages per time:\")\n",
    "print(weighted_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59cfe6",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "    \n",
    "Broadcasting enables efficient operations between different shapes without memory copies:\n",
    "    \n",
    "![Broadcasting](figures/broadcasting.png)\n",
    "    \n",
    "The rules ensure memory efficiency while maintaining mathematical clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88a8969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.444619Z",
     "iopub.status.busy": "2025-01-09T15:33:14.444523Z",
     "iopub.status.idle": "2025-01-09T15:33:14.447814Z",
     "shell.execute_reply": "2025-01-09T15:33:14.447564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Calibrated (first day):\n",
      "tensor([22.5000, 23.1000, 21.8000])\n",
      "tensor([22.9500, 22.6380, 22.0180])\n",
      "Memory efficiency: input elements = 9, output elements = 9\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings across days with efficient broadcasting\n",
    "day_temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8]   # Day 3\n",
    "])\n",
    "\n",
    "# Sensor calibration factors (per time of day)\n",
    "calibration = torch.tensor([1.02, 0.98, 1.01])\n",
    "\n",
    "# Broadcasting: implicit expansion without memory allocation\n",
    "calibrated = day_temps * calibration  # Efficient in-place operation\n",
    "print(\"Original vs Calibrated (first day):\")\n",
    "print(day_temps[0])      # Before calibration\n",
    "print(calibrated[0])     # After calibration\n",
    "print(f\"Memory efficiency: input elements = {day_temps.numel()}, output elements = {calibrated.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbe350",
   "metadata": {},
   "source": [
    "### Memory Layout\n",
    "    \n",
    "Understanding memory layout is crucial for performance:\n",
    "    \n",
    "![Memory Layout](figures/memory_layout.png)\n",
    "    \n",
    "Row-major storage affects operation speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8014a328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.449484Z",
     "iopub.status.busy": "2025-01-09T15:33:14.449373Z",
     "iopub.status.idle": "2025-01-09T15:33:14.451810Z",
     "shell.execute_reply": "2025-01-09T15:33:14.451532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row access stride: (1,)\n",
      "Column access stride: (3,)\n",
      "Memory layout: True\n"
     ]
    }
   ],
   "source": [
    "# Memory access patterns affect performance\n",
    "day_readings = week_temps[0]        # Fast: contiguous memory\n",
    "morning_temps = week_temps[:, 0]    # Slower: strided access\n",
    "\n",
    "# Demonstrate layout impact\n",
    "print(\"Row access stride:\", week_temps[0].stride())      # Small stride\n",
    "print(\"Column access stride:\", week_temps[:, 0].stride()) # Large stride\n",
    "print(\"Memory layout:\", week_temps.is_contiguous())      # Check if contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186728b",
   "metadata": {},
   "source": [
    "### Quick Check: Matrix Operations\n",
    "    \n",
    "Combine broadcasting and matrix multiplication efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf009249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.453407Z",
     "iopub.status.busy": "2025-01-09T15:33:14.453302Z",
     "iopub.status.idle": "2025-01-09T15:33:14.456466Z",
     "shell.execute_reply": "2025-01-09T15:33:14.456145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and weighted averages: tensor([22.4910, 22.4616, 21.7453])\n"
     ]
    }
   ],
   "source": [
    "# Temperature data with specified memory layout\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],\n",
    "    [21.0, 22.5, 20.9]\n",
    "], dtype=torch.float32)  # Ensure BLAS compatibility\n",
    "\n",
    "calibration = torch.tensor([1.02, 0.98, 1.01])  # Per-time calibration\n",
    "weights = torch.tensor([0.7, 0.3])              # Weights for each day\n",
    "\n",
    "# Fused operations for efficiency\n",
    "calibrated = temps * calibration                 # Broadcasting: (2,3) * (3,) -> (2,3)\n",
    "weighted_avg = calibrated.t() @ weights          # Matrix multiply: (3,2) @ (2,) -> (3,)\n",
    "print(\"Calibrated and weighted averages:\", weighted_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a1b62",
   "metadata": {},
   "source": [
    "## Finding Patterns with SVD\n",
    "\n",
    "SVD (Singular Value Decomposition) factorizes matrices into orthogonal components, enabling:\n",
    "1. Dimensionality reduction with provable optimality\n",
    "2. Noise filtering through low-rank approximation\n",
    "3. Pattern discovery in high-dimensional data\n",
    "    \n",
    "![SVD Decomposition](figures/svd_decomposition.png)\n",
    "\n",
    "Let's analyze how SVD helps with spam detection. First, let's look at some example emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20402960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.458221Z",
     "iopub.status.busy": "2025-01-09T15:33:14.458124Z",
     "iopub.status.idle": "2025-01-09T15:33:14.463799Z",
     "shell.execute_reply": "2025-01-09T15:33:14.463507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: tensor([4.3175e+02, 9.2474e+00, 1.7934e+00, 1.3037e+00, 1.2639e-01])\n",
      "\n",
      "Energy per pattern: tensor([9.9951e+01, 4.5852e-02, 1.7245e-03, 9.1130e-04, 8.5654e-06]) %\n"
     ]
    }
   ],
   "source": [
    "# Example emails\n",
    "emails = [\n",
    "    \"\"\"URGENT!! Make MONEY Fast! Buy our amazing product now!!!\n",
    "    Don't miss this INCREDIBLE opportunity to earn $$$$$\n",
    "    Click here: www.suspicious-link.com\"\"\",  # Spam\n",
    "    \n",
    "    \"\"\"CONGRATULATIONS! You've WON $10,000,000!!!\n",
    "    Send your bank details NOW to claim your PRIZE!!\n",
    "    Visit: www.totally-legit-money.com/claim\"\"\",  # Spam\n",
    "    \n",
    "    \"\"\"Dear valued customer, Your ACCOUNT needs verification!\n",
    "    URGENT: Click here to prevent account suspension!!!\n",
    "    www.bank-security-verify.com\"\"\",  # Spam\n",
    "    \n",
    "    \"\"\"ACT NOW!!! Limited time offer - 90% OFF!!!\n",
    "    Premium watches and luxury items at INCREDIBLE prices!\n",
    "    Order here: www.discount-luxury-items.com\"\"\",  # Spam\n",
    "    \n",
    "    \"\"\"ATTENTION: Your payment was declined!!!\n",
    "    Update your billing information IMMEDIATELY!\n",
    "    www.account-verify-now.com\"\"\",  # Spam\n",
    "    \n",
    "    \"\"\"Hi team, Here's the quarterly report for Q3 2023.\n",
    "    Please review the attached spreadsheet and let me know\n",
    "    if you have any questions.\"\"\",  # Not spam\n",
    "    \n",
    "    \"\"\"Meeting reminder: Project sync tomorrow at 10am.\n",
    "    Agenda items: 1. Sprint review 2. Planning\n",
    "    Please come prepared with your updates.\"\"\",  # Not spam\n",
    "    \n",
    "    \"\"\"Thank you for your order #12345.\n",
    "    Your package has been shipped and will arrive in 2-3 days.\n",
    "    Track your delivery at: shipping.legitimate-store.com\"\"\",  # Not spam\n",
    "    \n",
    "    \"\"\"Weekly team newsletter: \n",
    "    1. New hire welcome\n",
    "    2. Office updates\n",
    "    3. Upcoming events\"\"\",  # Not spam\n",
    "    \n",
    "    \"\"\"Your library book is due in 3 days.\n",
    "    Please return or renew online at library.edu/renew\n",
    "    Thank you for using our services.\"\"\"  # Not spam\n",
    "]\n",
    "\n",
    "# Feature extraction functions\n",
    "def count_exclamations(text):\n",
    "    \"\"\"Count exclamation marks\"\"\"\n",
    "    return text.count('!')\n",
    "\n",
    "def count_urgent_words(text):\n",
    "    \"\"\"Count urgent words like 'urgent', 'now', 'act'\"\"\"\n",
    "    urgent = ['urgent', 'now', 'act', 'immediate', 'fast']\n",
    "    return sum(text.lower().count(word) for word in urgent)\n",
    "\n",
    "def count_suspicious_links(text):\n",
    "    \"\"\"Count suspicious links (simplified)\"\"\"\n",
    "    suspicious = ['.com/', 'click', 'www.']\n",
    "    return sum(text.lower().count(marker) for marker in suspicious)\n",
    "\n",
    "def compute_caps_ratio(text):\n",
    "    \"\"\"Compute ratio of uppercase to total letters\"\"\"\n",
    "    letters = sum(c.isalpha() for c in text)\n",
    "    if letters == 0:\n",
    "        return 0\n",
    "    caps = sum(c.isupper() for c in text)\n",
    "    return caps / letters\n",
    "\n",
    "def get_length(text):\n",
    "    \"\"\"Get text length\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "# Extract features from emails\n",
    "features = []\n",
    "for email in emails:\n",
    "    features.append([\n",
    "        count_exclamations(email),\n",
    "        count_urgent_words(email),\n",
    "        count_suspicious_links(email),\n",
    "        compute_caps_ratio(email),\n",
    "        get_length(email)\n",
    "    ])\n",
    "\n",
    "# Convert to tensor\n",
    "X = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Analyze with SVD\n",
    "U, S, V = torch.linalg.svd(X)\n",
    "print(\"Singular values:\", S)\n",
    "print(\"\\nEnergy per pattern:\", 100 * S**2 / torch.sum(S**2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daae1b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The decomposition reveals:\n",
    "1. Feature patterns (V): Which features occur together\n",
    "2. Email patterns (U): How emails combine features\n",
    "3. Pattern strengths (S): How important each pattern is\n",
    "\n",
    "Looking at the first pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c522b7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.465477Z",
     "iopub.status.busy": "2025-01-09T15:33:14.465358Z",
     "iopub.status.idle": "2025-01-09T15:33:14.468463Z",
     "shell.execute_reply": "2025-01-09T15:33:14.468177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature pattern: tensor([-0.0206, -0.0076, -0.0061, -0.0010, -0.9997])\n",
      "Email pattern: tensor([-0.3546, -0.3245, -0.3314, -0.3408, -0.2758, -0.3219, -0.3219, -0.3543,\n",
      "        -0.2153, -0.2964])\n",
      "\n",
      "Second feature pattern: tensor([ 0.9283,  0.2724,  0.2503,  0.0304, -0.0227])\n",
      "Second email pattern: tensor([ 0.3693,  0.3426,  0.1339,  0.4278,  0.1956, -0.3123, -0.3417, -0.3762,\n",
      "        -0.2285, -0.3147])\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature pattern:\", V[0])\n",
    "print(\"Email pattern:\", U[:, 0])\n",
    "\n",
    "# Look at second pattern for spam detection\n",
    "print(\"\\nSecond feature pattern:\", V[1])\n",
    "print(\"Second email pattern:\", U[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56470d1d",
   "metadata": {},
   "source": [
    "This dominant pattern shows:\n",
    "1. Features: The first singular vector is dominated by text length (-0.9997), with negligible contributions from other features. When weighted by the large first singular value (431.75), this indicates that text length is the primary distinguishing feature.\n",
    "\n",
    "2. Email patterns: The first left singular vector shows similar weights across all emails (around -0.3), indicating that text length alone does not effectively separate spam from non-spam. This makes sense - both spam and legitimate emails can be long or short.\n",
    "\n",
    "3. Looking at the second singular value (9.2474) and its corresponding vectors:\n",
    "   ```python\n",
    "   print(\"Second feature pattern:\", V[1])\n",
    "   print(\"Second email pattern:\", U[:, 1])\n",
    "   ```\n",
    "   This reveals that exclamation marks and urgent words are more discriminative. The second singular vector in V shows larger weights for these spam-indicative features, and the corresponding U vector more clearly separates spam (first 5 emails) from non-spam (last 5 emails).\n",
    "\n",
    "Now let's analyze temperature patterns using PyTorch's highly optimized SVD implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c1097d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.470039Z",
     "iopub.status.busy": "2025-01-09T15:33:14.469957Z",
     "iopub.status.idle": "2025-01-09T15:33:14.473112Z",
     "shell.execute_reply": "2025-01-09T15:33:14.472860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: tensor([86.6684,  0.5942,  0.2883])\n",
      "\n",
      "Energy per pattern: tensor([9.9994e+01, 4.6999e-03, 1.1066e-03]) %\n",
      "\n",
      "Numerical rank: 3\n"
     ]
    }
   ],
   "source": [
    "# Week of temperature readings\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8],  # Day 3\n",
    "    [22.8, 23.5, 21.9],  # Day 4\n",
    "    [21.5, 22.8, 21.2]   # Day 5\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Compute SVD using optimized LAPACK routines\n",
    "U, S, V = torch.linalg.svd(temps)\n",
    "print(\"Singular values:\", S)\n",
    "print(\"\\nEnergy per pattern:\", 100 * S**2 / torch.sum(S**2), \"%\")\n",
    "print(\"\\nNumerical rank:\", torch.linalg.matrix_rank(temps).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284605e",
   "metadata": {},
   "source": [
    "The decomposition reveals the temperature data's intrinsic dimensionality:\n",
    "1. Feature patterns (V): Principal temperature variation modes\n",
    "2. Day patterns (U): How each day combines these modes\n",
    "3. Pattern strengths (S): Relative importance of each mode\n",
    "\n",
    "The rapid decay of singular values indicates low intrinsic dimensionality - daily temperatures follow strong physical constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a760849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.474660Z",
     "iopub.status.busy": "2025-01-09T15:33:14.474578Z",
     "iopub.status.idle": "2025-01-09T15:33:14.477366Z",
     "shell.execute_reply": "2025-01-09T15:33:14.477072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of day pattern (row 1 of V): tensor([-0.5726, -0.5982, -0.5606])\n",
      "Day pattern (column 1 of U): tensor([-0.4491, -0.4292, -0.4658, -0.4545, -0.4365])\n",
      "\n",
      "Cumulative variance explained: tensor([ 99.9942,  99.9989, 100.0000]) %\n"
     ]
    }
   ],
   "source": [
    "# Analyze the dominant pattern\n",
    "print(\"Time of day pattern (row 1 of V):\", V[0])\n",
    "print(\"Day pattern (column 1 of U):\", U[:, 0])\n",
    "\n",
    "# Compute percentage of variance explained\n",
    "total_var = torch.sum(S**2)\n",
    "explained_var = torch.cumsum(S**2, dim=0) / total_var\n",
    "print(\"\\nCumulative variance explained:\", 100 * explained_var, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602aa7c7",
   "metadata": {},
   "source": [
    "### Pattern Analysis Example: Checkerboard\n",
    "    \n",
    "A synthetic example reveals how SVD decomposes structured patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "146b4b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.478917Z",
     "iopub.status.busy": "2025-01-09T15:33:14.478806Z",
     "iopub.status.idle": "2025-01-09T15:33:14.481819Z",
     "shell.execute_reply": "2025-01-09T15:33:14.481583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: tensor([5.0012e+02, 3.0011e+02, 1.7744e-01, 4.7424e-02])\n",
      "\n",
      "Energy per pattern: tensor([7.3525e+01, 2.6475e+01, 9.2553e-06, 6.6112e-07]) %\n",
      "\n",
      "Effective numerical rank: 4\n"
     ]
    }
   ],
   "source": [
    "# Create checkerboard pattern with controlled noise\n",
    "pattern = torch.tensor([\n",
    "    [200,  50, 200,  50],\n",
    "    [ 50, 200,  50, 200],\n",
    "    [200,  50, 200,  50],\n",
    "    [ 50, 200,  50, 200]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Add small random noise to test stability\n",
    "noisy_pattern = pattern + torch.randn_like(pattern) * 0.1\n",
    "\n",
    "# Analyze clean vs noisy patterns\n",
    "U, S, V = torch.linalg.svd(noisy_pattern)\n",
    "print(\"Singular values:\", S)\n",
    "print(\"\\nEnergy per pattern:\", 100 * S**2 / torch.sum(S**2), \"%\")\n",
    "print(\"\\nEffective numerical rank:\", torch.sum(S > 1e-10).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e3119",
   "metadata": {},
   "source": [
    "The SVD reveals the checkerboard's structure:\n",
    "1. First component: Overall intensity (constant background)\n",
    "2. Second component: Alternating pattern (checkerboard)\n",
    "3. Remaining components: Numerical noise (~10⁻¹⁴)\n",
    "    \n",
    "This clean separation demonstrates SVD's power in pattern extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529925bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.483665Z",
     "iopub.status.busy": "2025-01-09T15:33:14.483570Z",
     "iopub.status.idle": "2025-01-09T15:33:14.487218Z",
     "shell.execute_reply": "2025-01-09T15:33:14.486956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original first row: tensor([199.9576,  50.0306, 199.9225,  50.0035])\n",
      "Rank 1 first row: tensor([124.9520, 124.9886, 124.8831, 124.9990])\n",
      "Rank 2 first row: tensor([200.0140,  50.0284, 199.8661,  50.0058])\n",
      "\n",
      "Reconstruction error:\n",
      "Rank 1: 300.106\n",
      "Rank 2: 0.184\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct with different ranks using efficient matrix operations\n",
    "def reconstruct(U, S, V, k):\n",
    "    # Efficient reconstruction avoiding full matrix materialization\n",
    "    return (U[:, :k] @ (torch.diag(S[:k]) @ V[:k, :]))\n",
    "\n",
    "# Compare reconstructions\n",
    "rank1 = reconstruct(U, S, V, 1)  # Background only\n",
    "rank2 = reconstruct(U, S, V, 2)  # Full pattern\n",
    "\n",
    "print(\"Original first row:\", noisy_pattern[0])\n",
    "print(\"Rank 1 first row:\", rank1[0])\n",
    "print(\"Rank 2 first row:\", rank2[0])\n",
    "print(\"\\nReconstruction error:\")\n",
    "print(f\"Rank 1: {torch.norm(noisy_pattern - rank1, p='fro'):.3f}\")\n",
    "print(f\"Rank 2: {torch.norm(noisy_pattern - rank2, p='fro'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56676e",
   "metadata": {},
   "source": [
    "### Measuring Pattern Quality\n",
    "    \n",
    "The Frobenius norm provides a principled way to measure reconstruction quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbbe64d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.488821Z",
     "iopub.status.busy": "2025-01-09T15:33:14.488710Z",
     "iopub.status.idle": "2025-01-09T15:33:14.491550Z",
     "shell.execute_reply": "2025-01-09T15:33:14.491316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise:    583.1\n",
      "As vector:       583.1\n",
      "From SVD:        583.3\n",
      "\n",
      "Random rank-2 error: 584.6\n",
      "SVD rank-2 error:    0.4\n"
     ]
    }
   ],
   "source": [
    "# Three equivalent computations demonstrating numerical stability\n",
    "print(f\"Element-wise:    {torch.sqrt((pattern**2).sum()):.1f}\")\n",
    "print(f\"As vector:       {pattern.view(-1).norm(p=2):.1f}\")\n",
    "print(f\"From SVD:        {torch.norm(S, p=2):.1f}\")\n",
    "\n",
    "# Demonstrate optimality of SVD approximation\n",
    "def random_rank2(shape):\n",
    "    \"\"\"Generate random rank-2 matrix\"\"\"\n",
    "    return torch.randn(shape[0], 2) @ torch.randn(2, shape[1])\n",
    "\n",
    "# Compare with random rank-2 approximation\n",
    "random_approx = random_rank2(pattern.shape)\n",
    "svd_approx = reconstruct(U, S, V, 2)\n",
    "\n",
    "print(f\"\\nRandom rank-2 error: {torch.norm(pattern - random_approx, p='fro'):.1f}\")\n",
    "print(f\"SVD rank-2 error:    {torch.norm(pattern - svd_approx, p='fro'):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b65875",
   "metadata": {},
   "source": [
    "### Summary\n",
    "    \n",
    "PyTorch implements linear algebra through three key mechanisms, each optimized for performance:\n",
    "\n",
    "1. Tensors: Flexible N-dimensional arrays\n",
    "   - Hardware-accelerated operations\n",
    "   - Automatic memory management\n",
    "   - Efficient data movement\n",
    "\n",
    "2. Broadcasting: Implicit shape matching\n",
    "   - Zero-copy operations\n",
    "   - Cache-friendly access patterns\n",
    "   - Automatic parallelization\n",
    "\n",
    "3. SVD: Pattern discovery and compression\n",
    "   - LAPACK-optimized implementation\n",
    "   - Numerically stable algorithms\n",
    "   - Automatic workspace management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fd2161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:33:14.493247Z",
     "iopub.status.busy": "2025-01-09T15:33:14.493140Z",
     "iopub.status.idle": "2025-01-09T15:33:14.496025Z",
     "shell.execute_reply": "2025-01-09T15:33:14.495793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Essential operations summary with performance notes\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)  # Contiguous memory allocation\n",
    "y = torch.zeros_like(x)                           # Pre-allocated memory\n",
    "z = torch.randn(3, 3)                            # Vectorized random generation\n",
    "A = z                                            # Already float32 from randn\n",
    "\n",
    "b = x + 2                                        # Fused operation\n",
    "c = torch.dot(x, x)                             # BLAS optimized\n",
    "d = A @ x                                       # Matrix multiply (GEMV)\n",
    "e = torch.mean(A, dim=0)                        # Stable reduction\n",
    "\n",
    "f = A.t()                                       # View only - no copy\n",
    "g = A.view(-1)                                  # Reshape without copy\n",
    "h = A[:, :2]                                    # Efficient slicing\n",
    "\n",
    "U, S, V = torch.linalg.svd(A)                   # LAPACK optimized\n",
    "norm = torch.norm(x, p=2)                       # Stable computation"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
