{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "el4kTByntJLs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6J1OYDMtWdQ",
    "outputId": "a64d7d3e-3a9a-4809-ca17-0d98f333e5ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "##TODO: Use dataloader on train and test dataset for future training\n",
    "## each time only one batch will be fed into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQRBKUGMtkn0",
    "outputId": "0348b3fd-e502-4696-efa1-7f9667b2f64b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## TODO: Please construct a network using 2 conv, 2 ReLU, 2 Maxpool Layer to half the size, 2 Fully Connected layers\n",
    "## and define the forward function\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,32,kernel_size=3, padding=1)\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=3, padding=1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.fc1=nn.Linear(64*8*8,100);\n",
    "        self.fc2=nn.Linear(100,10);\n",
    "    def forward(self, x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.fc2(F.relu(self.fc1(x)))\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "## TODO: Please manually calculate the number of trainable parameters you construct and compare with by writing a code\n",
    "\n",
    "## TODO: Define your own loss function and optimizer\n",
    "crit=nn.CrossEntropyLoss();\n",
    "#optimizer=optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data,label=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pNHEsUhjt3Qj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training Loss: 1.3534\n",
      "Epoch 1, training Acc: 0.5187\n",
      "Epoch 1, testing Loss: 1.0771\n",
      "Epoch 1, tesing Acc: 0.5938\n",
      "Epoch 2, training Loss: 0.9804\n",
      "Epoch 2, training Acc: 0.6576\n",
      "Epoch 2, testing Loss: 0.9005\n",
      "Epoch 2, tesing Acc: 0.7266\n",
      "Epoch 3, training Loss: 0.8413\n",
      "Epoch 3, training Acc: 0.7062\n",
      "Epoch 3, testing Loss: 0.7347\n",
      "Epoch 3, tesing Acc: 0.7578\n",
      "Epoch 4, training Loss: 0.7426\n",
      "Epoch 4, training Acc: 0.7396\n",
      "Epoch 4, testing Loss: 0.5967\n",
      "Epoch 4, tesing Acc: 0.8047\n",
      "Epoch 5, training Loss: 0.6590\n",
      "Epoch 5, training Acc: 0.7723\n",
      "Epoch 5, testing Loss: 0.5859\n",
      "Epoch 5, tesing Acc: 0.8047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    size=0\n",
    "    count=0\n",
    "    for images, labels in train_dataloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        pred=model(images)        \n",
    "        loss=crit(pred,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        size+=images.shape[0]\n",
    "        count+=1\n",
    "        running_loss+=loss.item()\n",
    "        running_accuracy+=(pred.argmax(dim=1)==labels).float().sum().item()\n",
    "        ##TODO: Training the model and calculate the loss and accuracy\n",
    "    print(f\"Epoch {epoch+1}, training Loss: {running_loss/count:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, training Acc: {running_accuracy/size:.4f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        size=0\n",
    "        count=0\n",
    "        for images, labels in test_dataloader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            images=data.to(device)\n",
    "            labels=label.to(device)\n",
    "            pred=model(images)\n",
    "            loss=crit(pred,labels)\n",
    "            size+=images.shape[0]\n",
    "            count+=1\n",
    "            running_loss+=loss.item()\n",
    "            running_accuracy+=(pred.argmax(dim=1)==labels).float().sum().item()\n",
    "    print(f\"Epoch {epoch+1}, testing Loss: {running_loss/count:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, tesing Acc: {running_accuracy/size:.4f}\")\n",
    "        ## TODO: Test the model on test data by starting with model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"CIFAR10_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0, nesterov=False):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "        if nesterov and momentum <= 0:\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum\")\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            for p in group[\"params\"]:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "\n",
    "                if \"momentum_buffer\" not in state.keys():\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                g = g.add(buf, alpha=momentum) if group[\"nesterov\"] else buf\n",
    "\n",
    "                p.data.mul_(len(p.data)**0.5 / p.data.norm()) # normalize the weight\n",
    "                update = zeropower_via_newtonschulz5(g.reshape(len(g), -1)).view(g.shape) # whiten the update\n",
    "                p.data.add_(update, alpha=-lr) # take a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect parameters\n",
    "muon_params = list(model.fc1.parameters()) + list(model.fc2.parameters())\n",
    "adam_params = [p for n, p in model.named_parameters() if not any(layer in n for layer in [\"fc1\", \"fc2\"])]\n",
    "# example Adam optimizer\n",
    "adam = torch.optim.Adam(adam_params, lr=1e-3)\n",
    "\n",
    "# assume you have a Muon optimizer class (like from Muon repo)\n",
    "#from muon import Muon  # <- replace with actual import\n",
    "muon = Muon(muon_params, lr=0.02, momentum=0.95, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training Loss: 0.7807\n",
      "Epoch 1, training Acc: 0.7315\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    size=0\n",
    "    count=0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # zero grads\n",
    "        adam.zero_grad()\n",
    "        muon.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        pred = model(images)\n",
    "        loss = crit(pred, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step both optimizers\n",
    "        adam.step()\n",
    "        muon.step()\n",
    "        size+=images.shape[0]\n",
    "        count+=1\n",
    "        running_loss+=loss.item()\n",
    "        running_accuracy+=(pred.argmax(dim=1)==labels).float().sum().item()\n",
    "            ##TODO: Training the model and calculate the loss and accuracy\n",
    "    print(f\"Epoch {epoch+1}, training Loss: {running_loss/count:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, training Acc: {running_accuracy/size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
