{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17560e87-5171-41e3-a1a1-406e3ac489c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self Attention Model\n",
    "\\begin{align}\n",
    "Q &= X W^Q, \\\\\n",
    "K &= X W^K, \\\\\n",
    "V &= X W^V, \\\\\n",
    "\\text{Attention}(Q, K, V) \n",
    "  &= \\operatorname{Softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{MultiHead}(X) \n",
    "  &= \\text{Concat}\\big(\\text{head}_1, \\ldots, \\text{head}_h\\big) W^O, \\\\\n",
    "\\text{head}_i \n",
    "  &= \\operatorname{Attention}(X W_i^Q, X W_i^K, X W_i^V).\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eacefb-df00-4907-8f8c-70e2f562f068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size is: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class myselfattention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length,bias=False):\n",
    "        super().__init__()\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=bias)\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=bias)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "    def forward(self, x): \n",
    "        _,num_tokens,_=x.shape\n",
    "        value=self.W_value(x)\n",
    "        query=self.W_query(x)\n",
    "        key=self.W_key(x)\n",
    "        attn_scores=key@query.mT\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)      \n",
    "        return nn.Softmax(dim=-1)(attn_scores/key.shape[-1]**0.5)@value\n",
    "myattention=myselfattention(4,4,3)\n",
    "input=torch.rand(2,3,4)\n",
    "output=myattention(input)\n",
    "print(f\"output size is: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c21a2b3-f7e6-459e-b86f-08a893b3ad81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myselfattention_mh(nn.Module):\n",
    "    def __init__(self, d_in, d_head, d_out,context_length,bias=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([myselfattention(d_in, d_out//d_head, context_length) for _ in range(d_head)])\n",
    "        self.W_out=nn.Linear(d_out,d_out,bias)\n",
    "    def forward(self, x): \n",
    "        out=torch.cat([self.heads[i](x) for i in range(len(self.heads))], dim=-1)\n",
    "        return self.W_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617281bd-8ece-4c31-a22d-7c421eed1431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(myselfattention_mh(16,8,16,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7889b4-5490-4aab-a67f-e0b331533efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class mySilu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+0.044715*x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5e2d2a-f000-47c5-bf46-9b437e112b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ffn(nn.Module):\n",
    "    def __init__(self,dim,dropout=0.1,bias=False):\n",
    "        super().__init__()\n",
    "        self.W1=nn.Linear(dim,4*dim,bias)\n",
    "        self.W2=nn.Linear(4*dim,dim,bias)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.silu=mySilu()\n",
    "    def forward(self, x): \n",
    "        return self.dropout(self.W2(self.silu(self.W1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa43e59c-0735-4394-b0ec-a434ea343ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myTransformer(nn.Module):\n",
    "    def __init__(self, heads, dropout, hidden,context_length,bias=False):\n",
    "        super().__init__()\n",
    "        self.myselfattention_mh=myselfattention_mh(hidden,heads,hidden,context_length)#nn.ModuleList([myselfattention(d_in, d_head) for _ in range(d_out//d_head)])\n",
    "        self.layernorm1=nn.LayerNorm(hidden)\n",
    "        self.layernorm2=nn.LayerNorm(hidden)\n",
    "        self.ffn=ffn(hidden,dropout,bias)\n",
    "        self.dropout=dropout\n",
    "    def forward(self, x): \n",
    "        shortcut1=x;\n",
    "        x=self.layernorm1(x)\n",
    "        x=self.myselfattention_mh(x)\n",
    "        x=nn.Dropout(self.dropout)(x)\n",
    "        x=x+shortcut1\n",
    "        shortcut2=x;\n",
    "        x=self.layernorm2(x)\n",
    "        x=self.ffn(x)\n",
    "        x=nn.Dropout(self.dropout)(x)\n",
    "        x=x+shortcut2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff897df7-3b17-409f-8365-5f05bf6d154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\n",
    "    \"layers\": 12,\n",
    "    \"heads\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"context_length\": 1024,\n",
    "    \"hidden_dim\": 768,\n",
    "    \"voc_size\": 50257\n",
    "}\n",
    "class myGPT2(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers=cfg[\"layers\"]\n",
    "        self.heads=cfg[\"heads\"]\n",
    "        self.dropout=cfg[\"dropout\"]\n",
    "        self.context_length=cfg[\"context_length\"]\n",
    "        self.hidden_dim=cfg[\"hidden_dim\"]\n",
    "        self.voc_size=cfg[\"voc_size\"]\n",
    "        self.wte=nn.Embedding(self.voc_size,self.hidden_dim)\n",
    "        self.wpe=nn.Embedding(self.context_length,self.hidden_dim)\n",
    "        self.layernorm=nn.LayerNorm(self.hidden_dim)\n",
    "        self.linear=nn.Linear(self.hidden_dim,self.voc_size,bias=False)\n",
    "        self.transformerBlocks=nn.ModuleList([myTransformer(self.heads,self.dropout,self.hidden_dim,self.context_length) for _ in range(self.layers)])\n",
    "    def forward(self, x): \n",
    "        batch,length=x.shape\n",
    "        te=self.wte(x);\n",
    "        pe=self.wpe(torch.arange(length));\n",
    "        x=te+pe;\n",
    "        x=nn.Dropout(self.dropout)(x)\n",
    "        x=nn.Sequential(*self.transformerBlocks)(x)\n",
    "        x=self.layernorm(x)\n",
    "        x=self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4525ab-4f99-4396-95bc-212d096a85e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "model=myGPT2(cfg)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5119c2c1-f8e9-46b3-895e-05ca57187f26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162954240"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7059baf-17fb-40f7-8426-ee3285679cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kail/.conda/envs/myenv/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_text1=\"I study at Clemson\"\n",
    "input_text2=\"I am a student\"\n",
    "input=[input_text1,input_text2]\n",
    "import tiktoken\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#text = \"Hello, I live in Atlanta.\"\n",
    "tokens = [enc.encode(input_text) for input_text in input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c6b0fa-6b62-40d4-a12c-f395d209131a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[40, 2050, 379, 27801], [40, 716, 257, 3710]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49debf0e-5215-43f4-b81a-1cccb8323ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(tokens)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067116a2-f62c-4453-8650-d0020168d802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model,tokens,max_length):\n",
    "    for i in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            x=model(tokens)\n",
    "        x_pred=x[:,-1,:]\n",
    "        _,max_ind=torch.max(x_pred, -1, keepdim=True)\n",
    "        tokens=torch.cat([tokens, max_ind], dim=1)\n",
    "    return tokens\n",
    "generated=generate(model,torch.tensor(tokens),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "712cb47a-bf75-4f3b-ac2d-358ad5c44a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  2050,   379, 27801, 34473, 15460, 44910, 21511, 20771, 29794],\n",
       "        [   40,   716,   257,  3710, 11818, 24990, 19440, 43833, 22896, 36284]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8bcd6f1-2ecb-409c-b6db-db446fcde032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: I study at Clemsonヘラlat Witches weekends educateuko\n",
      "Sequence 1: I am a student Tan Scientology severityFYworkers vehement\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(generated.tolist()):\n",
    "    text = enc.decode(seq)\n",
    "    print(f\"Sequence {i}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a812abb-467f-4adb-9c2a-7228ca480483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 716, 257, 3710, 11818, 24990, 19440, 43833, 22896, 36284]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa6c7e-52fb-41c8-8551-855149b5d96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
